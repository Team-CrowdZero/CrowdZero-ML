import requests
from bs4 import BeautifulSoup
import re
import time
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# ê¸°ë³¸ URL ì„¤ì •
base_url = "https://www.smpa.go.kr"
list_page_url = f"{base_url}/user/nd54882.do"

# User-Agent ì„¤ì •(í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36"
}

# Selenium WebDriver ì„¤ì •
options = Options()
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
service = Service()
driver = webdriver.Chrome(service=service, options=options)

# ëª©ë¡ í˜ì´ì§€ ìš”ì²­
response = requests.get(list_page_url, headers=headers, timeout=10)
soup = BeautifulSoup(response.text, "lxml")

# ìƒì„¸ í˜ì´ì§€ URL ê°€ì ¸ì˜¤ê¸°
detail_links = soup.select("table tr td:nth-of-type(2) a")
detail_page_urls = []

for link in detail_links:
    href = link["href"]
    match = re.search(r"goBoardView\('(/user/nd54882\.do)','View','(\d+)'\)", href)
    if match:
        actual_url = f"{base_url}{match.group(1)}?View&boardNo={match.group(2)}"
        detail_page_urls.append(actual_url)

print(f"ìƒì„¸ í˜ì´ì§€ ê°œìˆ˜: {len(detail_page_urls)}")

# ìƒì„¸ í˜ì´ì§€ URLì„ íŒŒì¼ë¡œ ì €ì¥
output_text_dir = r"C:\\Users\\USER\\Downloads\\web_crwl\\download_url"
os.makedirs(output_text_dir, exist_ok=True)
print(f"URL ì €ì¥ë¨: {output_text_dir}")

text_file_path = os.path.join(output_text_dir, "detail_urls.txt")
with open(text_file_path, "w", encoding="utf-8") as text_file:
    for detail_url in detail_page_urls:
        text_file.write(detail_url + "\n")

print(f"ìƒì„¸ í˜ì´ì§€ URL ì €ì¥ ì™„ë£Œ: {text_file_path}")

# ì €ì¥ëœ URL íŒŒì¼ì„ ì½ì–´ì„œ Seleniumìœ¼ë¡œ PDF ë‹¤ìš´
pdf_output_dir = r"C:\\Users\\USER\Desktop\\project_crwl\\web_crwl\\download_pdf"
os.makedirs(pdf_output_dir, exist_ok=True)

with open(text_file_path, "r", encoding="utf-8") as file:
    urls = file.readlines()

for idx, url in enumerate(urls):
    url = url.strip()
    if not url:
        continue
    
    print(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
    try:
        driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # PDF íŒŒì¼ë§Œ ë‹¤ìš´
        try:
            pdf_buttons = driver.find_elements(By.XPATH, "//a[contains(@onclick, 'attachfileDownload') and contains(text(), '.pdf')]")
            for pdf_button in pdf_buttons:
                pdf_name = pdf_button.text.strip().replace("/", "_")  # íŒŒì¼ëª…ì—ì„œ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì ì œê±°
                pdf_url = pdf_button.get_attribute("onclick")
                match = re.search(r"attachfileDownload\('(/common/attachfile/attachfileDownload.do)','(\d+)'\)", pdf_url)
                if match:
                    pdf_download_url = f"{base_url}{match.group(1)}?attachNo={match.group(2)}"
                    print(f"PDF ë§í¬ ë°œê²¬: {pdf_download_url}")
                    pdf_response = requests.get(pdf_download_url, headers=headers, stream=True)
                    if pdf_response.status_code == 200:
                        pdf_path = os.path.join(pdf_output_dir, f"{pdf_name}")
                        with open(pdf_path, "wb") as pdf_file:
                            for chunk in pdf_response.iter_content(chunk_size=1024):
                                if chunk:
                                    pdf_file.write(chunk)
                        print(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_path}")
                    else:
                        print(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {pdf_download_url}")
        except Exception as e:
            print(f"PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url} ({e})")
    except Exception as e:
        print(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} ({e})")

driver.quit()
print(f"ëª¨ë“  PDFê°€ {pdf_output_dir}ì— ì €ì¥ë¨.")


import os
import pandas as pd
import pdfplumber
import re

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_input_dir = r"C:\\Users\\USER\Desktop\\project_crwl\\web_crwl\\download_pdf"
csv_output_dir = r"C:\\Users\\USER\Desktop\\project_crwl\\web_crwl\\output_txt_csv"
os.makedirs(csv_output_dir, exist_ok=True)

# í•œì ë³€í™˜ dict
char_replacements = {
    "å‡º": "ë²ˆ ì¶œêµ¬",
    "æœªå®š": "ë¯¸ì •"
}

# í–‰ì •êµ¬ì—­ íŒ¨í„´ (<> ì•ˆì— ìˆëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ)
location_pattern = re.compile(r"<([^<>]+)>")

# PDFì—ì„œ í‘œ ì¶”ì¶œ ë° CSV ì €ì¥
for pdf_file in os.listdir(pdf_input_dir):
    if pdf_file.endswith(".pdf"):
        pdf_path = os.path.join(pdf_input_dir, pdf_file)
        csv_output_path = os.path.join(csv_output_dir, pdf_file.replace(".pdf", ".csv"))

        try:
            with pdfplumber.open(pdf_path) as pdf:
                all_tables = []

                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        df = pd.DataFrame(table)

                        # ì»¬ëŸ¼ ê°œìˆ˜ í™•ì¸ (5ê°œ ì»¬ëŸ¼ ìœ ì§€)
                        if df.shape[1] < 5:
                            continue  
                        df = df.iloc[:, :5]  # ì²« 5ê°œ ì»¬ëŸ¼ë§Œ ìœ ì§€

                        # ì»¬ëŸ¼ëª… ì •ë¦¬
                        df.columns = ["ì§‘íšŒ ì¼ì‹œ", "ì§‘íšŒ ì¥ì†Œ(í–‰ì§„ë¡œ)", "ì‹ ê³  ì¸ì›", "ê´€í• ì„œ", "ë¹„ê³ "]

                        # í–‰ì •êµ¬ì—­(ë™) ë¶„ë¦¬
                        df["í–‰ì •êµ¬ì—­(ë™)"] = df["ì§‘íšŒ ì¥ì†Œ(í–‰ì§„ë¡œ)"].apply(lambda x: location_pattern.search(str(x)).group(1) if location_pattern.search(str(x)) else "")
                        df["ì§‘íšŒ ì¥ì†Œ(í–‰ì§„ë¡œ)"] = df["ì§‘íšŒ ì¥ì†Œ(í–‰ì§„ë¡œ)"].apply(lambda x: location_pattern.sub("", str(x)).strip())

                        # í•œì ë³€í™˜ ì²˜ë¦¬
                        df = df.applymap(lambda x: ''.join([char_replacements.get(c, c) for c in str(x)]) if isinstance(x, str) else x)

                        all_tables.append(df)

                # ëª¨ë“  í˜ì´ì§€ì—ì„œ í‘œë¥¼ í•©ì³ì„œ CSV ì €ì¥
                if all_tables:
                    final_df = pd.concat(all_tables, ignore_index=True)

                    # 0í–‰ ì œê±° (ì»¬ëŸ¼ëª…ì´ ì¤‘ë³µë  ê°€ëŠ¥ì„±)
                    final_df = final_df.iloc[1:]

                    # CSV ì €ì¥
                    final_df.to_csv(csv_output_path, index=False, encoding="utf-8-sig")
                    print(f"CSV ì €ì¥ ì™„ë£Œ: {csv_output_path}")
                else:
                    print(f"í‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")

        except Exception as e:
            print(f"í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} ({e})")

print(f"ëª¨ë“  PDFì—ì„œ ì§‘íšŒ ì¼ì •ì´ {csv_output_dir}ì— ì €ì¥ë¨.")

import requests
import pandas as pd
import glob
import os

#CSV íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
CSV_FOLDER = r"C:\Users\USER\Desktop\project_crwl\web_crwl\output_txt_csv"
API_URL = "http://127.0.0.1:8000/add_protest"  # FastAPI ì„œë²„ ì£¼ì†Œ

def send_csv_to_api():
    csv_files = glob.glob(os.path.join(CSV_FOLDER, "*.csv"))  # ëª¨ë“  CSV íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    protests = []

    # CSV íŒŒì¼ í™•ì¸ ë° ë°ì´í„° ì½ê¸°
    if not csv_files:
        print("CSV íŒŒì¼ì´ ì—†ìŒìŒ")
        return

    for file in csv_files:
        df = pd.read_csv(file)  # CSV íŒŒì¼ ì½ê¸°
        print(f"ğŸ“„ {file}ì—ì„œ ë°ì´í„° ì½ìŒ:")
        print(df.head())  # CSV ë‚´ìš© í™•ì¸

        # NaN ê°’ Noneìœ¼ë¡œ ë³€í™˜ (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
        df = df.where(pd.notna(df), None)

        #ëª¨ë“  ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (JSON ë³€í™˜ ë¬¸ì œ ë°©ì§€)
        df = df.astype(str)

        # DataFrameì„ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        protests.extend(df.to_dict(orient="records"))

    # FastAPIë¡œ ë°ì´í„° ì „ì†¡
    if protests:
        print(f"ğŸ“¤ {len(protests)}ê°œ ë°ì´í„° ì „ì†¡ ì¤‘")
        try:
            response = requests.post(API_URL, json={"protests": protests})
            print(f"ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.json()}")
        except requests.exceptions.RequestException as e:
            print(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    send_csv_to_api()  # CSV ë°ì´í„° FastAPIì— ì „ì†¡

